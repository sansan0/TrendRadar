# coding=utf-8
"""
WeWork Compact Notification Extension

Transforms verbose notification messages into a compact format optimized for
WeWork and other channels with strict size limits.

Features:
- AI-powered cross-news summary (requires AI_ANALYSIS enabled in main config)
- Compact title listing with platform sources: "titleA (å¾®åš/æŠ–éŸ³) / titleB (çŸ¥ä¹Ž)"
- Fallback mode when AI is unavailable: direct title extraction
- Appends existing AI analysis section when available

Configuration (config/extensions/wework_compact.yaml):
    enabled: true
    target_channels:
      - wework
    compact_summary:
      summary_prefix: "ä»Šæ—¥çƒ­ç‚¹æ€»ç»“:"
      source_prefix: "æ¥æº:"
      fallback_prefix: "ä»Šæ—¥çƒ­ç‚¹:"
      separator: " / "
      max_titles: 20
      include_ai_analysis: true
    ai:
      use_main_config: true
      prompt_file: "wework_compact_prompt.txt"
    fallback:
      max_title_length: 25
      max_platform_display: 3
"""

import os
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests
from loguru import logger

from extensions.base import NotificationEnhancer


class WeWorkCompactPlugin(NotificationEnhancer):
    """
    Compact notification formatter for WeWork and similar channels.

    Transforms verbose news reports into a compact format:
    - With AI: "ä»Šæ—¥çƒ­ç‚¹æ€»ç»“: {AIç”Ÿæˆçš„ç»¼åˆæ‘˜è¦}\næ¥æº: titleA (platforms) / titleB (platforms)..."
    - Fallback: "ä»Šæ—¥çƒ­ç‚¹: titleA (platforms) / titleB (platforms)..."
    """

    @property
    def name(self) -> str:
        return "wework_compact"

    @property
    def version(self) -> str:
        return "1.0.0"

    def __init__(self):
        self.config: Dict[str, Any] = {}
        self._enabled = True
        self._target_channels: List[str] = ["wework"]
        self._compact_config: Dict[str, Any] = {}
        self._ai_config: Dict[str, Any] = {}
        self._ai_summary_config: Dict[str, Any] = {}
        self._fallback_config: Dict[str, Any] = {}

    def apply_config(self, config: Dict) -> None:
        """Apply plugin configuration from YAML file."""
        if config is None:
            config = {}
        self.config = config

        try:
            self._enabled = config.get("enabled", True) if config else True
            self._target_channels = config.get("target_channels", ["wework"])
            self._compact_config = config.get("compact_summary", {})
            self._ai_config = config.get("ai", {})
            self._fallback_config = config.get("fallback", {})

            # Load AI summary config with defaults
            ai_summary_config = self._ai_config.get("ai_summary", {})
            self._ai_summary_config = {
                "target_length": ai_summary_config.get("target_length", 250),
                "tone": ai_summary_config.get("tone", "journalistic"),
                "focus": ai_summary_config.get("focus", "facts"),
                "style": ai_summary_config.get("style", "concise"),
            }

            logger.info(
                "[wework_compact] Loaded plugin v{} (enabled={}, channels={})",
                self.version,
                self._enabled,
                self._target_channels,
            )
        except Exception as e:
            logger.error("[wework_compact] Error in apply_config: {}", e)
            self._enabled = False

    def enhance(
        self,
        content: str,
        channel: str,
        config: Dict[str, Any],
        context: Any,
    ) -> str:
        """
        Enhance notification content by replacing it with compact format.

        Args:
            content: Original notification content string
            channel: Notification channel name (e.g., 'wework')
            config: Plugin configuration
            context: Application context (AppContext instance)

        Returns:
            Compact formatted content string, or original content if not applicable
        """
        if not self._enabled:
            return content

        if channel not in self._target_channels:
            return content

        try:
            # Extract NEW titles with platforms from context
            titles_with_platforms = self._extract_titles_with_platforms(context, only_new=True)
            if not titles_with_platforms:
                logger.debug("[wework_compact] No new titles found in context")
                return content

            # Get main AI config to check if AI is enabled
            main_config = getattr(context, 'config', {})
            ai_analysis_config = self._get_ai_config(main_config)

            # Get existing AI analysis content
            existing_ai_analysis = self._get_existing_ai_analysis(context)

            # Generate compact content
            if ai_analysis_config:
                # Try AI mode
                compact_content = self._generate_ai_mode_content(
                    titles_with_platforms,
                    ai_analysis_config,
                    existing_ai_analysis,
                )
                if compact_content:
                    logger.info("[wework_compact] Generated AI-powered compact content")
                    return compact_content
                else:
                    logger.warning("[wework_compact] AI mode failed, falling back to simple mode")

            # Fallback mode
            compact_content = self._fallback_format(
                titles_with_platforms,
                existing_ai_analysis,
            )
            logger.info("[wework_compact] Generated fallback compact content")
            return compact_content

        except Exception as e:
            logger.error("[wework_compact] Error enhancing content: {}", e)
            return content

    def _extract_titles_with_platforms(
        self, context: Any, only_new: bool = False
    ) -> List[Tuple[str, List[str]]]:
        """
        Extract titles with their platform sources from context.

        Args:
            context: Application context
            only_new: If True, only extract titles that are in new_titles

        Returns:
            List of (title, [platform1, platform2, ...]) tuples
        """
        results: List[Tuple[str, List[str]]] = []
        seen_titles: set = set()

        # Try to get report_data from context
        report_data = getattr(context, 'report_data', None)
        if report_data is None:
            # Try to get from context dict if it's a dict-like object
            if hasattr(context, 'get'):
                report_data = context.get('report_data', {})
            else:
                return results

        if not report_data:
            return results

        # Build set of new titles if filtering
        new_titles_set: set = set()
        if only_new:
            new_titles_data = report_data.get('new_titles', [])
            for source_data in new_titles_data:
                if isinstance(source_data, dict):
                    for title_info in source_data.get('titles', []):
                        title = title_info.get('title') if isinstance(title_info, dict) else None
                        if title:
                            new_titles_set.add(title)

        # Extract from stats (keyword-grouped titles)
        stats = report_data.get('stats', [])
        for stat in stats:
            titles = stat.get('titles', [])
            for title_info in titles:
                if not isinstance(title_info, dict):
                    continue

                title = title_info.get('title', '')
                if not title:
                    continue

                # Clean title - remove URLs and markdown links
                title_clean = self._strip_markdown_links(title)
                title_clean = self._strip_urls(title_clean)
                title_clean = title_clean.strip()

                if not title_clean or title_clean in seen_titles:
                    continue

                # Filter to only new titles if requested
                if only_new and title_clean not in new_titles_set:
                    continue

                seen_titles.add(title_clean)

                # Get platforms this title appeared on
                platforms = title_info.get('platforms', [])
                if not platforms:
                    platform = title_info.get('platform', '')
                    source_name = title_info.get('source_name', '')
                    if platform:
                        platforms = [platform]
                    elif source_name:
                        platforms = [source_name]

                # Convert platform IDs to readable names if needed
                platforms = self._normalize_platform_names(platforms)

                results.append((title_clean, platforms))

        return results

    def _normalize_platform_names(self, platforms: List[str]) -> List[str]:
        """Normalize platform names to readable short names."""
        # Map common platform IDs to short names
        name_map = {
            'weibo': 'å¾®åš',
            'zhihu': 'çŸ¥ä¹Ž',
            'baidu': 'ç™¾åº¦',
            'douyin': 'æŠ–éŸ³',
            'bilibili': 'Bç«™',
            'toutiao': 'å¤´æ¡',
            '36kr': '36æ°ª',
            'ithome': 'ITä¹‹å®¶',
            'v2ex': 'V2EX',
            'juejin': 'æŽ˜é‡‘',
            'huxiu': 'è™Žå—…',
            'weixin': 'å¾®ä¿¡',
            'kuaishou': 'å¿«æ‰‹',
            'xiaohongshu': 'å°çº¢ä¹¦',
            'thepaper': 'æ¾Žæ¹ƒ',
        }

        normalized = []
        for p in platforms:
            if not p:
                continue
            p_lower = p.lower().strip()
            if p_lower in name_map:
                normalized.append(name_map[p_lower])
            else:
                # Keep original if not in map, but strip whitespace
                normalized.append(p.strip())

        return normalized

    def _strip_markdown_links(self, text: str) -> str:
        """Convert [title](url) to just title."""
        link_pattern = r'\[([^\]]+)\]\([^\)]+\)'
        return re.sub(link_pattern, r'\1', text)

    def _strip_urls(self, text: str) -> str:
        """Remove URLs from text."""
        url_pattern = r'https?://\S+|www\.\S+'
        return re.sub(url_pattern, '', text).strip()

    def _get_ai_config(self, main_config: Dict) -> Optional[Dict]:
        """
        Get AI config from main AI_ANALYSIS config section.

        Reuses the same AI provider/model as the main AI analysis feature.
        """
        if not self._ai_config.get('use_main_config', True):
            return None

        ai_analysis_config = main_config.get('AI_ANALYSIS', {})
        if not ai_analysis_config.get('ENABLED', False):
            return None

        # Check if API key is available
        api_key = ai_analysis_config.get('API_KEY') or os.environ.get('AI_API_KEY', '')
        if not api_key:
            return None

        return ai_analysis_config

    def _get_existing_ai_analysis(self, context: Any) -> Optional[str]:
        """Get the existing AI analysis content from context if available."""
        # Try multiple possible locations
        ai_content = getattr(context, 'ai_content', None)
        if ai_content:
            return ai_content

        ai_analysis = getattr(context, 'ai_analysis', None)
        if ai_analysis:
            # If it's an AIAnalysisResult object, extract formatted content
            if hasattr(ai_analysis, 'summary') and ai_analysis.summary:
                return self._format_ai_analysis_section(ai_analysis)

        return None

    def _format_ai_analysis_section(self, ai_analysis: Any) -> str:
        """Format AI analysis result into a readable section."""
        lines = []
        lines.append("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("ðŸ“Š AI æ·±åº¦åˆ†æž")
        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        if getattr(ai_analysis, 'summary', ''):
            lines.append(f"\nðŸ“Œ è¶‹åŠ¿æ¦‚è¿°\n{ai_analysis.summary}")

        if getattr(ai_analysis, 'keyword_analysis', ''):
            lines.append(f"\nðŸ”‘ å…³é”®è¯åˆ†æž\n{ai_analysis.keyword_analysis}")

        if getattr(ai_analysis, 'sentiment', ''):
            lines.append(f"\nðŸ’­ æƒ…æ„Ÿå€¾å‘\n{ai_analysis.sentiment}")

        if getattr(ai_analysis, 'cross_platform', ''):
            lines.append(f"\nðŸ”— è·¨å¹³å°å…³è”\n{ai_analysis.cross_platform}")

        if getattr(ai_analysis, 'signals', ''):
            lines.append(f"\nâš¡ å€¼å¾—å…³æ³¨\n{ai_analysis.signals}")

        if getattr(ai_analysis, 'conclusion', ''):
            lines.append(f"\nðŸ“ æ€»ç»“å»ºè®®\n{ai_analysis.conclusion}")

        return '\n'.join(lines)

    def _load_prompt_template(self) -> str:
        """Load the custom prompt template for compact summary generation."""
        prompt_file = self._ai_config.get('prompt_file', 'wework_compact_prompt.txt')
        config_dir = Path(__file__).parent.parent.parent / "config"
        prompt_path = config_dir / prompt_file

        if prompt_path.exists():
            return prompt_path.read_text(encoding='utf-8')

        # Build default prompt based on config
        return self._build_default_prompt()

    def _build_default_prompt(self) -> str:
        """Build default prompt dynamically from AI summary config."""
        tone_map = {
            "journalistic": "æ–°é—»æ’­æŠ¥é£Žæ ¼ï¼šå®¢è§‚ã€äº‹å®žæ€§ã€ç®€æ´",
            "analytical": "åˆ†æžè¯„è®ºé£Žæ ¼ï¼šæ·±å…¥è§£è¯»ã€æœ‰æ´žå¯ŸåŠ›",
            "conversational": "å£è¯­åŒ–é£Žæ ¼ï¼šè½»æ¾ã€æ˜“æ‡‚ã€äº²å’Œ",
        }

        focus_map = {
            "facts": "æ¦‚æ‹¬ä»Šæ—¥å‘ç”Ÿçš„æ ¸å¿ƒäº‹ä»¶å’Œäº‹å®ž",
            "analysis": "åˆ†æžäº‹ä»¶èƒŒåŽçš„åŽŸå› å’Œå½±å“",
            "trends": "æç‚¼æ•´ä½“è¶‹åŠ¿å’Œå…³è”",
            "all": "ç»¼åˆäº‹ä»¶ã€åˆ†æžå’Œè¶‹åŠ¿",
        }

        target_length = self._ai_summary_config.get("target_length", 250)
        tone_instruction = tone_map.get(
            self._ai_summary_config.get("tone", "journalistic"),
            tone_map["journalistic"]
        )
        focus_instruction = focus_map.get(
            self._ai_summary_config.get("focus", "facts"),
            focus_map["facts"]
        )

        prompt = f"""ä½ æ˜¯ä¸€ä½æ–°é—»ç¼–è¾‘ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼Œæ’°å†™ä¸€æ®µç®€çŸ­çš„æ–°é—»ç®€æŠ¥ï¼ˆçº¦{target_length}å­—ï¼Œ2-3å¥è¯ï¼‰ï¼Œæ¦‚æ‹¬ä»Šæ—¥å‘ç”Ÿçš„ä¸»è¦äº‹ä»¶ã€‚

è¦æ±‚ï¼š
1. é‡‡ç”¨{tone_instruction}
2. {focus_instruction}
3. ä½¿ç”¨æ–°é—»å¼€ç¯‡çš„å™è¿°æ–¹å¼ï¼ˆå¦‚"ä»Šæ—¥ï¼Œ..."ã€"æ®æŠ¥é“ï¼Œ..."ï¼‰
4. ä¸è¦é€æ¡åˆ—ä¸¾æ ‡é¢˜ï¼Œè€Œæ˜¯æç‚¼å‡ºæ•´ä½“äº‹ä»¶ç”»é¢
5. é¿å…ä¸»è§‚åˆ†æžå’Œè¯„è®ºï¼Œåªé™ˆè¿°äº‹å®ž
6. å¦‚æžœæœ‰å¤šä¸ªç‹¬ç«‹çƒ­ç‚¹ï¼Œç”¨åˆ†å·åˆ†éš”
7. è¯­è¨€ç®€ç»ƒï¼Œé¿å…å†—ä½™

æ–°é—»æ ‡é¢˜ï¼š
{{news_titles}}

è¯·ç›´æŽ¥è¾“å‡ºæ–°é—»ç®€æŠ¥å†…å®¹ï¼Œä¸éœ€è¦ä»»ä½•å‰ç¼€æˆ–æ ¼å¼æ ‡è®°ï¼š"""

        return prompt

    def _generate_cross_news_summary(
        self, titles: List[str], ai_config: Dict
    ) -> Optional[str]:
        """
        Generate a holistic summary across all news using AI.

        Uses the same AI provider as main AI_ANALYSIS config.
        """
        try:
            # Load prompt template
            prompt_template = self._load_prompt_template()

            # Build news titles string
            news_titles = '\n'.join(f"- {title}" for title in titles[:30])  # Limit to 30 titles
            prompt = prompt_template.replace('{news_titles}', news_titles)

            # Get API settings
            api_key = ai_config.get('API_KEY') or os.environ.get('AI_API_KEY', '')
            provider = ai_config.get('PROVIDER', 'openai')
            model = ai_config.get('MODEL', 'gpt-4o-mini')
            base_url = ai_config.get('BASE_URL', '')
            timeout = ai_config.get('TIMEOUT', 30)

            # Determine API endpoint
            if base_url:
                api_url = f"{base_url.rstrip('/')}/chat/completions"
            elif provider == 'deepseek':
                api_url = "https://api.deepseek.com/chat/completions"
            else:
                api_url = "https://api.openai.com/v1/chat/completions"

            # Make API request
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key}',
            }

            payload = {
                'model': model,
                'messages': [{'role': 'user', 'content': prompt}],
                'max_tokens': 200,
                'temperature': 0.7,
            }

            response = requests.post(
                api_url,
                headers=headers,
                json=payload,
                timeout=timeout,
            )
            response.raise_for_status()

            result = response.json()
            summary = result.get('choices', [{}])[0].get('message', {}).get('content', '')
            return summary.strip() if summary else None

        except Exception as e:
            logger.error("[wework_compact] AI summary generation failed: {}", e)
            return None

    def _format_sources_line(
        self, titles_with_platforms: List[Tuple[str, List[str]]]
    ) -> str:
        """Format the sources line: 'titleA (platformA/B) / titleB (platformC)...'"""
        max_titles = self._compact_config.get('max_titles', 20)
        max_title_length = self._fallback_config.get('max_title_length', 25)
        max_platform_display = self._fallback_config.get('max_platform_display', 3)
        separator = self._compact_config.get('separator', ' / ')

        parts = []
        for title, platforms in titles_with_platforms[:max_titles]:
            # Truncate title if needed
            if len(title) > max_title_length:
                title = title[:max_title_length - 1] + 'â€¦'

            # Format platforms
            if platforms:
                platform_str = '/'.join(platforms[:max_platform_display])
                parts.append(f"{title} ({platform_str})")
            else:
                parts.append(title)

        return separator.join(parts)

    def _generate_ai_mode_content(
        self,
        titles_with_platforms: List[Tuple[str, List[str]]],
        ai_config: Dict,
        existing_ai_analysis: Optional[str],
    ) -> Optional[str]:
        """Generate content in AI mode with cross-news summary."""
        # Extract just titles for AI summarization
        titles = [t[0] for t in titles_with_platforms]

        # Generate cross-news summary
        summary = self._generate_cross_news_summary(titles, ai_config)
        if not summary:
            return None

        # Build content with new format
        lines = []
        lines.append(f"ä»Šæ—¥çƒ­ç‚¹: {summary}")
        lines.append("")

        # Add each title as a bullet point
        max_titles = self._compact_config.get('max_titles', 20)
        max_platform_display = self._fallback_config.get('max_platform_display', 3)

        for title, platforms in titles_with_platforms[:max_titles]:
            if platforms:
                platform_str = '/'.join(platforms[:max_platform_display])
                lines.append(f"- {title} ({platform_str})")
            else:
                lines.append(f"- {title}")

        # Append existing AI analysis if configured
        include_ai_analysis = self._compact_config.get('include_ai_analysis', True)
        if include_ai_analysis and existing_ai_analysis:
            lines.append("")
            lines.append(existing_ai_analysis)

        return '\n'.join(lines)

    def _fallback_format(
        self,
        titles_with_platforms: List[Tuple[str, List[str]]],
        existing_ai_analysis: Optional[str],
    ) -> str:
        """Generate compact format without AI (fallback mode)."""
        # Build content with new format
        lines = []

        # Use simple fallback summary
        count = len(titles_with_platforms)
        summary = f"å…±å‘çŽ° {count} æ¡æ–°çƒ­ç‚¹" if count > 0 else "æš‚æ— æ–°çƒ­ç‚¹"
        lines.append(f"ä»Šæ—¥çƒ­ç‚¹: {summary}")
        lines.append("")

        # Add each title as a bullet point
        max_titles = self._compact_config.get('max_titles', 20)
        max_platform_display = self._fallback_config.get('max_platform_display', 3)

        for title, platforms in titles_with_platforms[:max_titles]:
            if platforms:
                platform_str = '/'.join(platforms[:max_platform_display])
                lines.append(f"- {title} ({platform_str})")
            else:
                lines.append(f"- {title}")

        # Append existing AI analysis if configured (even in fallback mode)
        include_ai_analysis = self._compact_config.get('include_ai_analysis', True)
        if include_ai_analysis and existing_ai_analysis:
            lines.append("")
            lines.append(existing_ai_analysis)

        return '\n'.join(lines)


# Plugin instance for entry point discovery
plugin = WeWorkCompactPlugin()
