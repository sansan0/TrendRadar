# TrendRadar AI è°ƒç”¨å®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [AI è°ƒç”¨æ¦‚è¿°](#ai-è°ƒç”¨æ¦‚è¿°)
2. [AI åŠŸèƒ½è¯¦è§£](#ai-åŠŸèƒ½è¯¦è§£)
3. [AI æ¨¡å‹é…ç½®](#ai-æ¨¡å‹é…ç½®)
4. [ä»£ç è°ƒç”¨åˆ†æ](#ä»£ç è°ƒç”¨åˆ†æ)
5. [GLM æ¨¡å‹æ¥å…¥æ¡ˆä¾‹](#glm-æ¨¡å‹æ¥å…¥æ¡ˆä¾‹)
6. [è‡ªæ‰˜ç®¡æ¨¡å‹é…ç½®](#è‡ªæ‰˜ç®¡æ¨¡å‹é…ç½®)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## AI è°ƒç”¨æ¦‚è¿°

### æ ¸å¿ƒæ¶æ„

TrendRadar ä½¿ç”¨ **LiteLLM** ç»Ÿä¸€æ¥å£è°ƒç”¨å¤šç§ AI æ¨¡å‹ï¼Œå®ç°ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TrendRadar ä¸»ç¨‹åº                   â”‚
â”‚  (__main__.py, context.py, dispatcher.py)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”œâ”€â†’ AI åˆ†æåŠŸèƒ½
                    â”‚   â””â†’ AIAnalyzer
                    â”‚       â””â†’ AIClient â”€â”€â†’ LiteLLM â”€â”€â†’ AI æ¨¡å‹
                    â”‚
                    â””â”€â†’ AI ç¿»è¯‘åŠŸèƒ½
                        â””â†’ AITranslator
                            â””â†’ AIClient â”€â”€â†’ LiteLLM â”€â”€â†’ AI æ¨¡å‹
```

### AI åŠŸèƒ½ä¸€è§ˆ

| åŠŸèƒ½ | æ¨¡å— | ç›®çš„ | è§¦å‘æ¡ä»¶ |
|------|------|------|---------|
| **AI åˆ†æ** | `ai/analyzer.py` | å¯¹çƒ­ç‚¹æ–°é—»è¿›è¡Œæ·±åº¦æƒ…æŠ¥åˆ†æ | `ai_analysis.enabled=true` |
| **AI ç¿»è¯‘** | `ai/translator.py` | å°†æ¨é€å†…å®¹ç¿»è¯‘ä¸ºå…¶ä»–è¯­è¨€ | é…ç½®ç¿»è¯‘è¯­è¨€ |

---

## AI åŠŸèƒ½è¯¦è§£

### åŠŸèƒ½ 1ï¼šAI åˆ†æï¼ˆai_analysisï¼‰

#### åŠŸèƒ½æè¿°
å¯¹çƒ­ç‚¹æ–°é—»è¿›è¡Œæ·±åº¦æƒ…æŠ¥åˆ†æï¼Œæä¾›ï¼š
- æ ¸å¿ƒçƒ­ç‚¹æ€åŠ¿æç‚¼
- èˆ†è®ºé£å‘äº‰è®®åˆ†æ
- å¼‚åŠ¨ä¸å¼±ä¿¡å·æ•æ‰
- RSS æ·±åº¦æ´å¯Ÿ
- ç ”åˆ¤ç­–ç•¥å»ºè®®

#### é…ç½®ä½ç½®
**æ–‡ä»¶**: [config/config.yaml:389-420](../config/config.yaml)

**é»˜è®¤é…ç½®**:
```yaml
ai_analysis:
  enabled: false                      # æ˜¯å¦å¯ç”¨ AI åˆ†æ
  max_news_for_analysis: 50           # å‚ä¸åˆ†æçš„æ–°é—»æ•°é‡ä¸Šé™
  include_rss: false                  # æ˜¯å¦åŒ…å« RSS å†…å®¹
  include_rank_timeline: true        # æ˜¯å¦ä¼ é€’å®Œæ•´æ’åæ—¶é—´çº¿
```

#### ç¯å¢ƒå˜é‡é…ç½®
```bash
# .env æ–‡ä»¶é…ç½®
AI_ANALYSIS_ENABLED=true              # å¯ç”¨ AI åˆ†æ
AI_API_KEY=your_api_key              # API å¯†é’¥
AI_MODEL=deepseek/deepseek-chat       # æ¨¡å‹åç§°
AI_API_BASE=                          # å¯é€‰ï¼šè‡ªå®šä¹‰ API ç«¯ç‚¹
```

#### æç¤ºè¯æ–‡ä»¶
**æ–‡ä»¶**: [config/ai_analysis_prompt.txt](../config/ai_analysis_prompt.txt)

**å†…å®¹ç»“æ„**:
```
ç³»ç»Ÿæç¤ºè¯ï¼š
- å®šä¹‰ AI è§’è‰²ï¼ˆæƒ…æŠ¥åˆ†æä¸“å®¶ï¼‰
- è§„å®šè¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
- è¯´æ˜åˆ†ææ¡†æ¶ï¼ˆ5ä¸ªæ ¸å¿ƒæ¿å—ï¼‰

ç”¨æˆ·æç¤ºè¯æ¨¡æ¿ï¼š
- çƒ­æ¦œç»Ÿè®¡æ•°æ®
- RSS è®¢é˜…æ•°æ®
- å…³é”®è¯ä¿¡æ¯
- å¹³å°ä¿¡æ¯
- æ—¶é—´èŒƒå›´
```

#### è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
```json
{
  "æ ¸å¿ƒçƒ­ç‚¹æ€åŠ¿": {
    "å…±æ€§æç‚¼": "...",
    "å®šæ€§åˆ¤æ–­": "..."
  },
  "èˆ†è®ºé£å‘äº‰è®®": {
    "æƒ…ç»ªå…‰è°±": "...",
    "è®¤çŸ¥æ–­å±‚": "..."
  },
  "å¼‚åŠ¨ä¸å¼±ä¿¡å·": {
    "è·¨å¹³å°å…±æŒ¯": ["..."],
    "è½¨è¿¹çªå˜": ["..."]
  },
  "RSSæ·±åº¦æ´å¯Ÿ": {
    "ä¸“ä¸šé¢†åŸŸç›²åŒº": ["..."]
  },
  "ç ”åˆ¤ç­–ç•¥å»ºè®®": {
    "å†³ç­–å±‚": "...",
    "æ‰§è¡Œå±‚": "...",
    "è§‚å¯Ÿå±‚": "..."
  }
}
```

---

### åŠŸèƒ½ 2ï¼šAI ç¿»è¯‘ï¼ˆai_translationï¼‰

#### åŠŸèƒ½æè¿°
å°†æ¨é€å†…å®¹ç¿»è¯‘ä¸ºæŒ‡å®šè¯­è¨€ï¼Œæ”¯æŒï¼š
- å•æ¡æ–‡æœ¬ç¿»è¯‘
- æ‰¹é‡æ–‡æœ¬ç¿»è¯‘
- ä¿æŒæ–°é—»ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§

#### é…ç½®ä½ç½®
**æ–‡ä»¶**: [config/config.yaml:428-437](../config/config.yaml)

**é»˜è®¤é…ç½®**:
```yaml
ai_translation:
  language: "English"                 # ç›®æ ‡è¯­è¨€
  batch_size: 10                       # æ‰¹é‡ç¿»è¯‘å¤§å°
```

#### æç¤ºè¯æ–‡ä»¶
**æ–‡ä»¶**: [config/ai_translation_prompt.txt](../config/ai_translation_prompt.txt)

**å†…å®¹**:
```
è§’è‰²ï¼šä¸“ä¸šç¿»è¯‘
è¦æ±‚ï¼šå‡†ç¡®ã€ä¸“ä¸šã€æµç•…
è¾“å‡ºï¼šç¿»è¯‘åçš„æ–‡æœ¬
```

---

## AI æ¨¡å‹é…ç½®

### é…ç½®æ–‡ä»¶

**ä¸»é…ç½®æ–‡ä»¶**: [config/config.yaml:324-381](../config/config.yaml)

```yaml
ai:
  # LiteLLM æ¨¡å‹æ ¼å¼: provider/model_name
  model: "deepseek/deepseek-chat"
                                  # å…¶ä»–ç¤ºä¾‹:
                                  # - openai/gpt-4o
                                  # - anthropic/claude-3-5-sonnet
                                  # - gemini/gemini-2.5-flash
                                  # - ollama/llama3

  # API å¯†é’¥ï¼ˆå»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
  api_key: ""                        # é€šè¿‡ AI_API_KEY ç¯å¢ƒå˜é‡è®¾ç½®

  # è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
  api_base: ""                       # è‡ªå®šä¹‰ API ç«¯ç‚¹ URL

  # è¶…æ—¶è®¾ç½®
  timeout: 120                       # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰

  # ç”Ÿæˆå‚æ•°
  temperature: 1.0                   # é‡‡æ ·æ¸©åº¦ï¼ˆ0.0-2.0ï¼‰
  max_tokens: 5000                   # æœ€å¤§ç”Ÿæˆ token æ•°

  # é‡è¯•é…ç½®
  num_retries: 1                     # å¤±è´¥é‡è¯•æ¬¡æ•°
  fallback_models: []                # å¤‡ç”¨æ¨¡å‹åˆ—è¡¨
                                # ["openai/gpt-4o-mini", "..."]
```

### æ”¯æŒçš„æ¨¡å‹æä¾›å•†

é¡¹ç›®é€šè¿‡ **LiteLLM** æ”¯æŒ 100+ AI æä¾›å•†ï¼š

| æä¾›å•† | æ¨¡å‹æ ‡è¯† | ç‰¹ç‚¹ |
|--------|---------|------|
| **DeepSeek** | `deepseek/deepseek-chat` | é»˜è®¤æ¨¡å‹ï¼Œæ€§ä»·æ¯”é«˜ |
| **OpenAI** | `openai/gpt-4o` | æœ€å¼ºå¤§çš„æ¨¡å‹ |
| **OpenAI** | `openai/gpt-4o-mini` | å¿«é€Ÿä¾¿å®œ |
| **Anthropic** | `anthropic/claude-3-5-sonnet` | é•¿æ–‡æœ¬å¤„ç†å¼º |
| **Google** | `gemini/gemini-2.5-flash` | é€Ÿåº¦å¿« |
| **Ollama** | `ollama/llama3` | æœ¬åœ°è¿è¡Œï¼Œå…è´¹ |
| **æ™ºè°± GLM** | `zhipu/glm-4` | å›½äº§æ¨¡å‹ |
| **é˜¿é‡Œäº‘** | `qwen/qwen-2.5` | å›½äº§æ¨¡å‹ |
| **Moonshot** | `moonshot/v1-8k` | Kimi |
| **ç™¾å·** | `baichuan/Baichuan2` | å›½äº§æ¨¡å‹ |

**å®Œæ•´åˆ—è¡¨**: https://docs.litellm.ai/docs/providers

---

## ä»£ç è°ƒç”¨åˆ†æ

### 1. AI å®¢æˆ·ç«¯ï¼ˆai/client.pyï¼‰

**æ–‡ä»¶**: [trendradar/ai/client.py](../trendradar/ai/client.py)

**æ ¸å¿ƒç±»**:
```python
class AIClient:
    """AI å®¢æˆ·ç«¯ï¼ŒåŸºäº LiteLLM"""

    def __init__(self, config):
        self.model = config.get("model") or os.environ.get("AI_MODEL", "")
        self.api_key = config.get("api_key") or os.environ.get("AI_API_KEY", "")
        self.api_base = config.get("api_base") or os.environ.get("AI_API_BASE", "")

    def chat(self, messages: list) -> str:
        """è°ƒç”¨ AI æ¨¡å‹è¿›è¡Œå¯¹è¯

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ [{"role": "user", "content": "..."}]

        Returns:
            str: AI è¿”å›çš„æ–‡æœ¬å†…å®¹
        """
        import litellm

        response = litellm.completion(
            model=self.model,
            messages=messages,
            api_key=self.api_key,
            api_base=self.api_base,
            timeout=self.timeout,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )

        return response["choices"][0]["message"]["content"]
```

---

### 2. AI åˆ†æå™¨ï¼ˆai/analyzer.pyï¼‰

**æ–‡ä»¶**: [trendradar/ai/analyzer.py](../trendradar/ai/analyzer.py)

**æ ¸å¿ƒæ–¹æ³•**:
```python
class AIAnalyzer:
    """AI åˆ†æå™¨"""

    def analyze(self, hotlist_stats, rss_stats, report_mode,
                report_type, display_regions):
        """æ‰§è¡Œ AI åˆ†æ

        Args:
            hotlist_stats: çƒ­æ¦œç»Ÿè®¡æ•°æ®
            rss_stats: RSS ç»Ÿè®¡æ•°æ®
            report_mode: æŠ¥å‘Šæ¨¡å¼
            report_type: æŠ¥å‘Šç±»å‹
            display_regions: æ˜¾ç¤ºçš„åœ°åŒºä¿¡æ¯

        Returns:
            AIAnalysisResult: åˆ†æç»“æœå¯¹è±¡
        """
        # 1. åŠ è½½æç¤ºè¯æ¨¡æ¿
        prompt_template = self._load_prompt_template()

        # 2. å‡†å¤‡æ–°é—»å†…å®¹
        news_content = self._prepare_news_content(
            hotlist_stats, rss_stats,
            self.max_news_for_analysis
        )

        # 3. æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = prompt_template.format(
            language="ç®€ä½“ä¸­æ–‡",
            report_mode=report_mode,
            report_type=report_type,
            current_time=time.strftime("%Y-%m-%d %H:%M:%S"),
            news_count=len(news_content),
            rss_count=rss_stats.total_items if rss_stats else 0,
            keywords=self._extract_keywords(news_content),
            platforms=self._get_platform_names(news_content),
            news_content=self._format_news_dict(news_content),
            rss_content=self._format_rss_dict(rss_stats)
        )

        # 4. è°ƒç”¨ AI
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        response = self.ai_client.chat(messages)

        # 5. è§£æ JSON å“åº”
        result = self._parse_response(response)

        return result
```

---

### 3. AI ç¿»è¯‘å™¨ï¼ˆai/translator.pyï¼‰

**æ–‡ä»¶**: [trendradar/ai/translator.py](../trendar/ai/translator.py)

**æ ¸å¿ƒæ–¹æ³•**:
```python
class AITranslator:
    """AI ç¿»è¯‘å™¨"""

    def translate(self, text: str, target_language: str = None) -> str:
        """ç¿»è¯‘å•æ¡æ–‡æœ¬

        Args:
            text: å¾…ç¿»è¯‘æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰

        Returns:
            str: ç¿»è¯‘åçš„æ–‡æœ¬
        """
        language = target_language or self.config.get("language", "English")

        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘ä¸º{language}ï¼š\n\n{text}"}
        ]

        # è°ƒç”¨ AI
        response = self.ai_client.chat(messages)

        return response.strip()

    def translate_batch(self, texts: List[str], target_language: str = None) -> List[str]:
        """æ‰¹é‡ç¿»è¯‘ï¼ˆæé«˜æ•ˆç‡ï¼‰

        Args:
            texts: å¾…ç¿»è¯‘æ–‡æœ¬åˆ—è¡¨
            target_language: ç›®æ ‡è¯­è¨€

        Returns:
            List[str]: ç¿»è¯‘åçš„æ–‡æœ¬åˆ—è¡¨
        """
        # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯ batch_size æ¡ï¼‰
        results = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]

            # æ„å»ºæ‰¹é‡è¯·æ±‚
            content = "\n---\n".join([f"{idx+1}. {text}" for idx, text in enumerate(batch)])

            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘ä¸º{language}ï¼Œæ¯è¡Œä¸€ä¸ªç¿»è¯‘ç»“æœï¼š\n\n{content}"}
            ]

            response = self.ai_client.chat(messages)

            # è§£ææ‰¹é‡ç»“æœ
            batch_results = self._parse_batch_response(response)
            results.extend(batch_results)

        return results
```

---

### 4. ä¸»ç¨‹åºè°ƒç”¨å…¥å£

#### AI åˆ†æå…¥å£ï¼ˆ__main__.py:247ï¼‰

```python
# AI åˆ†æåŠŸèƒ½è°ƒç”¨
if ai_analysis_enabled:
    from trendradar.ai.analyzer import AIAnalyzer

    analyzer = AIAnalyzer(config, logger)
    ai_analysis_result = analyzer.analyze(
        hotlist_stats=statistics,
        rss_stats=rss_statistics,
        report_mode=report_mode,
        report_type=report_type,
        display_regions=platform_regions
    )
```

#### AI ç¿»è¯‘å…¥å£ï¼ˆcontext.py:451ï¼‰

```python
# AI ç¿»è¯‘å™¨åˆå§‹åŒ–
if ai_translation_enabled:
    from trendradar.ai.translator import AITranslator

    translator = AITranslator(config, logger)
```

#### é€šçŸ¥åˆ†å‘ä¸­çš„ç¿»è¯‘ï¼ˆdispatcher.pyï¼‰

```python
# åœ¨æ¨é€æ—¶è°ƒç”¨ AI ç¿»è¯‘
if ai_translation_enabled:
    translated = translator.translate(original_text, "English")
```

---

## GLM æ¨¡å‹æ¥å…¥æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šä½¿ç”¨æ™ºè°± GLM-4

#### æ­¥éª¤ 1ï¼šæ³¨å†Œæ™ºè°± AI è´¦å·

1. è®¿é—®æ™ºè°± AI å¼€æ”¾å¹³å°ï¼šhttps://open.bigmodel.cn/
2. æ³¨å†Œè´¦å·å¹¶å®Œæˆå®åè®¤è¯
3. åˆ›å»º API Key

#### æ­¥éª¤ 2ï¼šé…ç½®ç¯å¢ƒå˜é‡

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```bash
# å¯ç”¨ AI åˆ†æ
AI_ANALYSIS_ENABLED=true

# æ™ºè°± GLM-4 é…ç½®
AI_API_KEY=your_zhipu_api_key_here
AI_MODEL=zhipu/glm-4
```

**å‚æ•°è¯´æ˜**ï¼š
- `AI_API_KEY`: æ™ºè°± API Keyï¼ˆä»å¹³å°è·å–ï¼‰
- `AI_MODEL`: æ¨¡å‹æ ‡è¯† `zhipu/glm-4`

#### æ­¥éª¤ 3ï¼šéªŒè¯é…ç½®

```bash
# é‡å¯å®¹å™¨
cd /soft/TrendRadar/docker
docker compose restart

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f trendradar

# æ‰‹åŠ¨æµ‹è¯•
docker exec -it trendradar python manage.py run
```

---

### æ¡ˆä¾‹ 2ï¼šä½¿ç”¨é˜¿é‡Œäº‘ Qwen

#### æ­¥éª¤ 1ï¼šå¼€é€šé˜¿é‡Œäº‘ç™¾ç‚¼æœåŠ¡

1. è®¿é—®é˜¿é‡Œäº‘ç™¾ç‚¼ï¼šhttps://bailian.console.aliyun.com/
2. å¼€é€šDashScopeæœåŠ¡
3. åˆ›å»º API Key

#### æ­¥éª¤ 2ï¼šé…ç½®ç¯å¢ƒå˜é‡

```bash
# å¯ç”¨ AI åˆ†æ
AI_ANALYSIS_ENABLED=true

# é˜¿é‡Œäº‘ Qwen é…ç½®
AI_API_KEY=your_alibaba_api_key_here
AI_MODEL=qwen/qwen-2.5
```

**å¯ç”¨æ¨¡å‹**ï¼š
- `qwen/qwen-2.5` - æœ€æ–°ç‰ˆæœ¬
- `qwen/qwen-turbo` - å¿«é€Ÿç‰ˆæœ¬
- `qwen/qwen-long` - é•¿æ–‡æœ¬ç‰ˆæœ¬

---

### æ¡ˆä¾‹ 3ï¼šä½¿ç”¨ Moonshot Kimi

#### æ­¥éª¤ 1ï¼šæ³¨å†Œ Moonshot

1. è®¿é—® Moonshotï¼šhttps://www.moonshot.cn/
2. æ³¨å†Œè´¦å·
3. åˆ›å»º API Key

#### æ­¥éª¤ 2ï¼šé…ç½®ç¯å¢ƒå˜é‡

```bash
# å¯ç”¨ AI åˆ†æ
AI_ANALYSIS_ENABLED=true

# Moonshot Kimi é…ç½®
AI_API_KEY=your_moonshot_api_key_here
AI_MODEL=moonshot/v1-8k
```

---

### æ¡ˆä¾‹ 4ï¼šä½¿ç”¨è‡ªæ‰˜ç®¡ GLMï¼ˆOllamaï¼‰

#### æ­¥éª¤ 1ï¼šå®‰è£… Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# ä¸‹è½½å®‰è£…åŒ…ï¼šhttps://ollama.com/download
```

#### æ­¥éª¤ 2ï¼šæ‹‰å– GLM æ¨¡å‹

```bash
# æ‹‰å– GLM-4 æ¨¡å‹
ollama pull glm4

# æˆ–æ‹‰å– ChatGLM3
ollama pull chatglm3
```

#### æ­¥éª¤ 3ï¼šå¯åŠ¨ Ollama æœåŠ¡

```bash
# å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# éªŒè¯æœåŠ¡è¿è¡Œ
curl http://localhost:11434/api/tags
```

#### æ­¥éª¤ 4ï¼šé…ç½® TrendRadar ä½¿ç”¨æœ¬åœ°æ¨¡å‹

**æ–¹æ³• 1ï¼šä¿®æ”¹ docker-compose.yml**

```yaml
services:
  trendradar:
    image: wantcat/trendradar:latest
    # ... å…¶ä»–é…ç½®

    # æ·»åŠ  extra_hostsï¼ˆè®©å®¹å™¨è®¿é—®å®¿ä¸»æœºï¼‰
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # é…ç½®ç¯å¢ƒå˜é‡
    environment:
      - AI_MODEL=ollama/glm4
      - AI_API_BASE=http://host.docker.internal:11434
      - AI_API_KEY=ollama  # Ollam ä¸éœ€è¦çœŸå®å¯†é’¥
```

**æ–¹æ³• 2ï¼šä½¿ç”¨ host ç½‘ç»œæ¨¡å¼**

```yaml
services:
  trendradar:
    image: wantcat/trendradar:latest
    network_mode: host  # ä½¿ç”¨å®¿ä¸»æœºç½‘ç»œ
    environment:
      - AI_MODEL=ollama/glm4
      - AI_API_BASE=http://localhost:11434
      - AI_API_KEY=ollama
```

#### æ­¥éª¤ 5ï¼šé‡å¯å®¹å™¨

```bash
docker compose down
docker compose up -d
```

---

## è‡ªæ‰˜ç®¡æ¨¡å‹é…ç½®

### ä½¿ç”¨ vLLM éƒ¨ç½² OpenAI å…¼å®¹ API

#### æ­¥éª¤ 1ï¼šå®‰è£… vLLM

```bash
# å®‰è£… vLLMï¼ˆéœ€è¦ Python 3.8+ï¼‰
pip install vllm

# æˆ–ä½¿ç”¨ Docker
docker pull vllm/vllm-openai:latest
```

#### æ­¥éª¤ 2ï¼šå¯åŠ¨ GLM æ¨¡å‹æœåŠ¡

```bash
# ä½¿ç”¨ vLLM å¯åŠ¨ GLM-4
python -m vllm.entrypoints.openai.api_server \
  --model THUDM/glm-4-9b-chat \
  --port 8000 \
  --host 0.0.0.0
```

**å‚æ•°è¯´æ˜**ï¼š
- `--model`: æ¨¡å‹è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ HuggingFace ID æˆ–æœ¬åœ°è·¯å¾„ï¼‰
- `--port`: æœåŠ¡ç«¯å£ï¼ˆé»˜è®¤ 8000ï¼‰
- `--host`: ç»‘å®šåœ°å€ï¼ˆé»˜è®¤ 0.0.0.0ï¼‰

#### æ­¥éª¤ 3ï¼šé…ç½® TrendRadar

```bash
# .env æ–‡ä»¶
AI_API_KEY=empty  # vLLM ä¸éœ€è¦å¯†é’¥
AI_MODEL=glm/glm-4-9b-chat  # ä½¿ç”¨æ¨¡å‹åç§°
AI_API_BASE=http://your_server_ip:8000/v1  # API ç«¯ç‚¹
```

---

## å¸¸è§æ¨¡å‹é…ç½®ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šæ™ºè°± GLM-4ï¼ˆAPIï¼‰

```bash
# .env é…ç½®
AI_ANALYSIS_ENABLED=true
AI_API_KEY=your_zhipu_api_key
AI_MODEL=zhipu/glm-4
AI_API_BASE=https://open.bigmodel.cn/api/paas/v4/
```

**æµ‹è¯•å‘½ä»¤**ï¼š
```bash
curl https://open.bigmodel.cn/api/paas/v4/chat/completions \
  -H "Authorization: Bearer your_zhipu_api_key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "glm-4",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ]
  }'
```

---

### ç¤ºä¾‹ 2ï¼šé˜¿é‡Œäº‘ Qwen 2.5ï¼ˆAPIï¼‰

```bash
# .env é…ç½®
AI_ANALYSIS_ENABLED=true
AI_API_KEY=your_alibaba_api_key
AI_MODEL=qwen/qwen-2.5
AI_API_BASE=https://dashscope.aliyuncs.com/compatible-mode/v1
```

**æµ‹è¯•å‘½ä»¤**ï¼š
```bash
curl https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \
  -H "Authorization: Bearer your_alibaba_api_key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-2.5",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ]
  }'
```

---

### ç¤ºä¾‹ 3ï¼šæœ¬åœ° GLM-4ï¼ˆOllamï¼‰

```bash
# .env é…ç½®
AI_ANALYSIS_ENABLED=true
AI_API_KEY=ollama
AI_MODEL=ollama/glm4
AI_API_BASE=http://localhost:11434
```

**æµ‹è¯•å‘½ä»¤**ï¼š
```bash
curl http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "glm4",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ],
    "stream": false
  }'
```

---

## å®Œæ•´é…ç½®æµç¨‹

### æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. é€‰æ‹©æ¨¡å‹  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ äº‘ç«¯ APIï¼ˆæ™ºè°±ã€é˜¿é‡Œäº‘ç­‰ï¼‰
       â”‚   1. æ³¨å†Œè´¦å·
â”‚   2. è·å– API Key
â”‚   3. é…ç½®ç¯å¢ƒå˜é‡
â”‚   4. é‡å¯å®¹å™¨
â”‚
       â””â”€â†’ æœ¬åœ°æ¨¡å‹ï¼ˆOllama/vLLMï¼‰
           1. å®‰è£… Ollama
           2. æ‹‰å–æ¨¡å‹
           3. å¯åŠ¨æœåŠ¡
           4. é…ç½®ç¯å¢ƒå˜é‡
           5. é‡å¯å®¹å™¨
```

### é…ç½®æ£€æŸ¥æ¸…å•

- [ ] ç¡®å®šä½¿ç”¨çš„ AI æ¨¡å‹
- [ ] è·å– API Keyï¼ˆäº‘ç«¯æ¨¡å‹ï¼‰æˆ–å®‰è£…æ¨¡å‹ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰
- [ ] é…ç½® `.env` æ–‡ä»¶
- [ ] éªŒè¯ API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
- [ ] é‡å¯å®¹å™¨
- [ ] æŸ¥çœ‹æ—¥å¿—éªŒè¯
- [ ] æ‰‹åŠ¨æ‰§è¡Œæµ‹è¯•

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šAI åˆ†ææ²¡æœ‰è§¦å‘

**æ£€æŸ¥**ï¼š
```bash
# 1. æ£€æŸ¥ç¯å¢ƒå˜é‡
docker exec -it trendradar python manage.py config

# 2. æŸ¥çœ‹æ—¥å¿—
docker compose logs trendradar | grep -i ai

# 3. æ£€æŸ¥é…ç½®æ–‡ä»¶
cat config/config.yaml | grep -A 10 "ai_analysis:"
```

**å¯èƒ½åŸå› **ï¼š
- `AI_ANALYSIS_ENABLED=false`
- `AI_API_KEY` æœªè®¾ç½®
- AI æ¨¡å‹é…ç½®é”™è¯¯

---

### é—®é¢˜ 2ï¼šAPI è°ƒç”¨å¤±è´¥

**æ£€æŸ¥**ï¼š
```bash
# 1. éªŒè¯ API Key
echo $AI_API_KEY

# 2. æµ‹è¯• API è¿æ¥
curl -X POST $AI_API_BASE/chat/completions \
  -H "Authorization: Bearer $AI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model": "$AI_MODEL", "messages": [{"role": "user", "content": "test"}]}'

# 3. æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—
docker compose logs trendradar | grep -i "error\|exception"
```

**å¸¸è§é”™è¯¯**ï¼š
- API Key æ— æ•ˆ
- API ç«¯ç‚¹é”™è¯¯
- æ¨¡å‹åç§°é”™è¯¯
- ç½‘ç»œè¿æ¥é—®é¢˜
- é…é¢ç”¨å°½

---

### é—®é¢˜ 3ï¼šå“åº”æ—¶é—´è¿‡é•¿

**è§£å†³**ï¼š

1. **é™ä½ `max_tokens`**
   ```yaml
   ai:
     max_tokens: 2000  # ä» 5000 é™ä½
   ```

2. **å‡å°‘åˆ†ææ–°é—»æ•°é‡**
   ```yaml
   ai_analysis:
     max_news_for_analysis: 30  # ä» 50 é™ä½
   ```

3. **ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹**
   ```bash
   AI_MODEL=openai/gpt-4o-mini  # æ¯”å®Œæ•´ç‰ˆå¿«
   ```

---

### é—®é¢˜ 4ï¼šè¾“å‡ºæ ¼å¼é”™è¯¯

**æ£€æŸ¥**ï¼š
```bash
# æŸ¥çœ‹ AI åŸå§‹å“åº”
docker compose logs trendradar | grep -A 20 "AI åˆ†æç»“æœ"

# éªŒè¯ JSON æ ¼å¼
```

**ä¿®å¤**ï¼š

1. ç¡®ä¿ AI æ¨¡å‹æ”¯æŒ JSON è¾“å‡º
2. åœ¨æç¤ºè¯ä¸­å¼ºè°ƒè¾“å‡º JSON æ ¼å¼
3. ä½¿ç”¨æ›´ç¨³å®šçš„æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰

---

## æˆæœ¬ä¼˜åŒ–

### æ§åˆ¶æˆæœ¬çš„æ–¹æ³•

#### 1. é™åˆ¶è¾“å…¥å¤§å°

```yaml
ai_analysis:
  max_news_for_analysis: 30  # å‡å°‘åˆ†æçš„æ–°é—»æ•°
```

#### 2. é™åˆ¶è¾“å‡ºé•¿åº¦

```yaml
ai:
  max_tokens: 2000  # å‡å°‘ max_tokens
```

#### 3. ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹

```bash
# é«˜æ€§ä»·æ¯”é€‰æ‹©
AI_MODEL=deepseek/deepseek-chat  # Â¥0.14/M tokens
AI_MODEL=zhipu/glm-4-flash     # Â¥0.05/M tokens
AI_MODEL=moonshot/v1-8k        # Â¥12/M tokens
```

#### 4. å‡å°‘è°ƒç”¨é¢‘ç‡

```yaml
# é™ä½å®šæ—¶ä»»åŠ¡é¢‘ç‡
CRON_SCHEDULE=*/60 * * * *  # ä» */30 æ”¹ä¸º */60
```

---

## é™„å½•ï¼šå®Œæ•´é…ç½®ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šä½¿ç”¨æ™ºè°± GLM-4

```ini
# .env æ–‡ä»¶
AI_ANALYSIS_ENABLED=true
AI_API_KEY=your_zhipu_api_key_here
AI_MODEL=zhipu/glm-4
AI_API_BASE=https://open.bigmodel.cn/api/paas/v4/
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=2000
```

### ç¤ºä¾‹ 2ï¼šä½¿ç”¨æœ¬åœ° Ollama

```ini
# .env æ–‡ä»¶
AI_ANALYSIS_ENABLED=true
AI_API_KEY=ollama
AI_MODEL=ollama/glm4
AI_API_BASE=http://host.docker.internal:11434
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=2000
```

### ç¤ºä¾‹ 3ï¼šä½¿ç”¨é˜¿é‡Œäº‘ Qwen

```ini
# .env æ–‡ä»¶
AI_ANALYSIS_ENABLED=true
AI_API_KEY=your_alibaba_api_key_here
AI_MODEL=qwen/qwen-2.5
AI_API_BASE=https://dashscope.aliyuncs.com/compatible-mode/v1
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=2000
```

---

## æ€»ç»“

### å…³é”®è¦ç‚¹

1. **ç»Ÿä¸€æ¥å£**ï¼šé€šè¿‡ LiteLLM æ”¯æŒ 100+ æ¨¡å‹
2. **çµæ´»é…ç½®**ï¼šæ”¯æŒäº‘ç«¯ API å’Œæœ¬åœ°æ¨¡å‹
3. **æˆæœ¬æ§åˆ¶**ï¼šé€šè¿‡å‚æ•°è°ƒæ•´ä¼˜åŒ–æˆæœ¬
4. **æ˜“äºåˆ‡æ¢**ï¼šæ›´æ¢æ¨¡å‹åªéœ€ä¿®æ”¹ç¯å¢ƒå˜é‡

### æ¨èé…ç½®

| åœºæ™¯ | æ¨èæ¨¡å‹ | æ€§ä»·æ¯” |
|------|---------|--------|
| **ç”Ÿäº§ç¯å¢ƒ** | `deepseek/deepseek-chat` | â­â­â­â­â­ |
| **å¿«é€Ÿæµ‹è¯•** | `openai/gpt-4o-mini` | â­â­â­â­ |
| **æœ¬åœ°éƒ¨ç½²** | `ollama/glm4` | â­â­â­â­â­ |
| **å›½å†…ä½¿ç”¨** | `zhipu/glm-4-flash` | â­â­â­â­â­ |

---

**é…ç½®æ„‰å¿«ï¼ğŸš€**

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
- [LiteLLM æ–‡æ¡£](https://docs.litellm.ai/)
- [æ™ºè°± AI å¹³å°](https://open.bigmodel.cn/)
- [é¡¹ç›®ä¸» README](../README.md)
